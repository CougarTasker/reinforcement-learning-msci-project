@article{zaouter2020autonomous,
  title     = {Autonomous systems in anesthesia: where do we stand in 2020? A narrative review},
  author    = {Zaouter, C{\'e}drick and Joosten, Alexandre and Rinehart, Joseph and Struys, Michel MRF and Hemmerling, Thomas M},
  journal   = {Anesthesia \& Analgesia},
  volume    = {130},
  number    = {5},
  year      = {2020},
  publisher = {Wolters Kluwer}
}

@article{hagele2016robots,
  title     = {Robots conquer the world [turning point]},
  author    = {Hagele, Martin},
  journal   = {IEEE Robotics \& Automation Magazine},
  volume    = {23},
  number    = {1},
  year      = {2016},
  publisher = {IEEE}
}

@article{franccois2018introduction,
  title     = {An introduction to deep reinforcement learning},
  author    = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G and Pineau, Joelle and others},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  volume    = {11},
  number    = {3-4},
  year      = {2018},
  publisher = {Now Publishers, Inc.}
}

@online{gym,
  title   = {Gym Documentation},
  url     = {https://www.gymlibrary.dev},
  journal = {www.gymlibrary.dev},
  author  = {OpenAI}
}
@online{pytorch,
  title   = {Pytorch Documentation},
  url     = {https://pytorch.org/docs/stable/index.html},
  journal = {pytorch.org},
  author  = {PyTorch Foundation}
}
@online{kivy,
  title   = {Kivy Documentation},
  url     = {https://kivy.org/doc/stable},
  journal = {kivy.org},
  author  = {Kivy Organization}
}
@online{numpy,
  title   = {NumPy Documentation},
  url     = {https://numpy.org/doc/stable},
  journal = {numpy.org},
}
@online{pandas,
  title   = {Pandas Documentation},
  url     = {https://pandas.pydata.org/docs},
  journal = {pydata.org},
}
@online{python,
  title   = {Python Documentation},
  url     = {https://docs.python.org/3/},
  journal = {python.org},
  author  = {Python Software Foundation}
}


@online{pypy2023benchmarks,
  title   = {PyPy performance benchmarks},
  url     = {https://speed.pypy.org/},
  journal = {pypy.org},
  author  = {The PyPy Team}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{raschka2020machine,
  title={Machine learning in python: Main developments and technology trends in data science, machine learning, and artificial intelligence},
  author={Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
  journal={Information},
  volume={11},
  number={4},
  pages={193},
  year={2020},
  publisher={MDPI}
}

@inproceedings{virbel2011kivy,
  title={Kivy--a framework for rapid creation of innovative user interfaces},
  author={Virbel, Mathieu and Hansen, Thomas and Lobunets, Oleksandr},
  booktitle={Workshop-Proceedings der Tagung Mensch \& Computer 2011. {\"u}berMEDIEN| {\"U}BERmorgen},
  year={2011},
  organization={Universit{\"a}tsverlag Chemnitz}
}

@article{ransbotham2017reshaping,
  title={Reshaping business with artificial intelligence: Closing the gap between ambition and action},
  author={Ransbotham, Sam and Kiron, David and Gerbert, Philipp and Reeves, Martin},
  journal={MIT Sloan Management Review},
  volume={59},
  number={1},
  year={2017},
  publisher={Massachusetts Institute of Technology, Cambridge, MA}
}

@book{mitchell1997machine,
  title={Machine Learning},
  author={Mitchell, T.M.},
  isbn={9780071154673},
  lccn={97007692},
  series={McGraw-Hill International Editions},
  year={1997},
  publisher={McGraw-Hill}
}

@article{ZOU2016372,
title = {Reinforcement learning-based real-time energy management for a hybrid tracked vehicle},
journal = {Applied Energy},
volume = {171},
pages = {372-382},
year = {2016},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2016.03.082},
url = {https://www.sciencedirect.com/science/article/pii/S0306261916304081},
author = {Yuan Zou and Teng Liu and Dexing Liu and Fengchun Sun},
keywords = {Hybrid tracked vehicle, Markov chain, Kullback–Leibler divergence rate, Reinforcement learning, Energy management, Control strategy},
abstract = {To realize the optimal energy allocation between the engine-generator and battery of a hybrid tracked vehicle (HTV), a reinforcement learning-based real-time energy-management strategy was proposed. A systematic control-oriented model for the HTV was built and validated through the test bench, including the battery pack, the engine-generator set (EGS), and the power request. To use effectively the statistical information of power request online, a Markov chain-based real-time power request recursive algorithm for learning transition probabilities was derived and validated. The Kullback–Leibler (KL) divergence rate was adopted to determine when the transition probability matrix and the optimal control strategy update in real time. Reinforcement learning (RL) was applied to compare quantitatively the effects of different forgetting factors and KL divergence rates on reducing fuel consumption. RL has also been used to optimize the control strategy for HTV, compared to preliminary and dynamic programming-based control strategies. The real-time and robust performance of the proposed online energy management strategy was verified under two driving schedules collected in the field test. The simulation results indicate the proposed RL-based energy management strategy can significantly improve fuel efficiency and can be applied in real time.}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}